# Modified SRResNet w/o BN from:
# Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network

# ----------- Commands for running
# ----------- Single GPU with auto_resume
# PYTHONPATH="./:${PYTHONPATH}"  CUDA_VISIBLE_DEVICES=0 python basicsr/train.py -opt options/train/SRResNet_SRGAN/train_MSRResNet_x4.yml --auto_resume

# 001_MSRResNet_x4_f64b16_DIV2K_1000k_B16G1_wandb
# * 001: 标号，方便进行实验管理
# * MSRResNet: 模型名称
# * x4: 放大4倍
# * f64b16: 中间feature通道数是16，使用16个Residual Block
# * DIV2K: 训练数据集是DIV2K
# * 1000k: 训练了1000k iterations
# * wandb: 使用wandb，训练过程上传了wandb服务器

# general settings
# 实验名称，若实验名称中有debug，则会进入debug模式
name: 001_MSRResNet_x4_f64b16_DIV2K_1000k_B16G1_wandb
# 使用的model类型
model_type: SRModel
# 上采样系数；若有些任务没有这个配置，则写1
scale: 4
# 指定使用的GPU卡数
num_gpu: 1 # set num_gpu: 0 for cpu mode; set auto to inference GPU
# 指定随机种子
manual_seed: 0

# dataset and data loader settings
datasets:
  train: # 训练集
    name: DIV2K # 自定义的数据集名称
    type: PairedImageDataset # 读取数据的Dataset类
    # 以下属性是灵活的，可在相应类的说明文档中获得。新加的数据集可根据需要添加
    # GT(Ground-Truth)图像的文件夹路径
    dataroot_gt: datasets/DF2K/DIV2K_train_HR_sub
    # LQ(Low-Quality)输入图像的文件夹路径
    dataroot_lq: datasets/DF2K/DIV2K_train_LR_bicubic_X4_sub
    # 预先生成的 meta_info 文件
    meta_info_file: basicsr/data/meta_info/meta_info_DIV2K800sub_GT.txt
    # (for lmdb)
    # dataroot_gt: datasets/DIV2K/DIV2K_train_HR_sub.lmdb
    # dataroot_lq: datasets/DIV2K/DIV2K_train_LR_bicubic_X4_sub.lmdb
    # 文件名称模板，一般LQ文件会有类似 '_x4' 这样的文件后缀，这个就是来处理GT和LQ文件后缀不匹配的问题
    filename_tmpl: "{}"
    io_backend:
      type: disk
      # (for lmdb)
      # type: lmdb

    # 预训练阶段裁剪（crop）的GT图像的尺寸大小，即训练的label大小
    gt_size: 128
    # 是否开启水平方向图像增强（随机水平翻转图像）
    use_hflip: true
    # 是否开启旋转图像增强（随机旋转图像）
    use_rot: true

    # data loader
    # 每个GPU的data loader读取进程数目
    num_worker_per_gpu: 6
    # 每个GPU上的batch size
    batch_size_per_gpu: 16
    # 放大dataset的长度倍数（默认为1），可以扩大一个epoch所需iterations
    # 例如，如果训练数据集有15张图，设置dataset_enlarge_ratio为100，那么程序
    # 会重复读取这些图片100次，这样一个epoch下来，便会读取1500张图。这个方法经常
    # 用来加速dataloader，因为有的机器上，一个epoch结束会重启进程，导致拖慢训练。
    dataset_enlarge_ratio: 100
    # 预先读取数据的方式
    # cpu表示使用CPU prefetcher
    # cuda表示使用CUDA prefetcher，他会多占用一些GPU显存。
    #       注意这个模式下，一定要设置pin_memory=True
    prefetch_mode: ~

  val:
    name: Set5
    type: PairedImageDataset
    dataroot_gt: datasets/Set5/GTmod12
    dataroot_lq: datasets/Set5/LRbicx4
    io_backend:
      type: disk

  val_2:
    name: Set14
    type: PairedImageDataset
    dataroot_gt: datasets/Set14/GTmod12
    dataroot_lq: datasets/Set14/LRbicx4
    io_backend:
      type: disk

# network structures
# 网络g的设置
network_g:
  type: MSRResNet # 网络结构（Architecture）的类型
  # 以下属性是灵活且特定的，可在相应类的说明文档中获得
  num_in_ch: 3 # 模型输入的图像通道数
  num_out_ch: 3 # 模型输出的图像通道数
  num_feat: 64 # 模型内部的 feature map 通道数
  num_block: 16 # 模型内部基础模块的堆叠数
  upscale: 4 # 上采样系数

# path
path:
  # 预训练模型的路径，需要以pth结尾的模型
  pretrain_network_g: ~
  # 读取预训练的参数key，若需要使用 EMA 模型，需要改成 params_ema
  param_key_g: params
  # 是否严格地根据参数名称一一对应load模型参数。
  # 如果选择false，那么模型对于找不到的参数，会随机初始化；
  # 如果选择true，假如存在不对应的参数，会报错提示
  strict_load_g: true
  # 重启训练的state路径，在experiments/exp_name/training_states目录下
  resume_state: ~

# training settings
train:
  ema_decay: 0.999 # EMA 更新权重
  optim_g: # 优化器的配置
    type: Adam # 优化器类型
    # !!float 是YAML语言语法，表示以float解释后面的数字，不然就会以文字进行解释
    lr: !!float 2e-4 # 初始学习率
    weight_decay: 0 # 权重衰退参数
    betas: [0.9, 0.99] # Adam 优化器的beta1和beta2

  scheduler: # 学习率调度器的配置
    type: CosineAnnealingRestartLR # 选择学习率更新策略
    # 以下属性是灵活的，根据学习率 scheduler 的不同有不同的设置
    periods: [250000, 250000, 250000, 250000] # Cosine Annealing 的更新周期
    restart_weights: [1, 1, 1, 1] # Cosine Annealing 每次 restart 的权重
    eta_min: !!float 1e-7 # 学习率衰退到最小值

  total_iter: 1000000 # 总共进行的训练迭代次数
  warmup_iter: -1 # no warm up

  # losses    损失函数的设置
  pixel_opt:
    type: L1Loss # loss 函数
    loss_weight: 1.0 # 指定loss的权重
    reduction: mean # loss reduction 方式

# validation settings
val: # validation 的配置
  val_freq: !!float 5e3 # validation频率，每隔5000 iterations做一次validation
  save_img: false # 是否在validation时保存图片

  metrics: # validation 中使用的指标的配置
    psnr: # metric name, can be arbitrary
      type: calculate_psnr # 选择指标类型
      # 以下属性是灵活的，根据不同metric有不同的设置
      crop_border: 4 # 计算指标时 crop图像边界像素范围（不纳入计算范围）
      test_y_channel: false # 是否转成在 Y(CbCr)空间上计算
      # 该指标是越高越好还是越低越好，选择higher或者lower
      better: higher # the higher, the better. Default: higher
    niqe:
      type: calculate_niqe
      crop_border: 4
      better: lower # the lower, the better

# logging settings
logger: # logging 的配置
  print_freq: 100 # 多少次迭代打印一次训练信息
  save_checkpoint_freq: !!float 5e3 # 多少次迭代保存一次模型权重和训练状态
  use_tb_logger: true # 是否使用tensorboad logger
  wandb: # 是否使用wandb logger
    project: ~ # wandb的project名字，默认是None，即不使用wanbd
    resume_id: ~ # 如果是resume，可以输入上次的wandb id，则log可以接起来

# dist training settings
dist_params: #  distributed training 的设置，目前只在Slurm训练下才需要
  backend: nccl
  port: 29500
